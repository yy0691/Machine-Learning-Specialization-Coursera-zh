{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 实践实验：用于手写数字识别的神经网络，多分类\n",
    "\n",
    "在本练习中，您将使用神经网络来识别手写数字 0-9。\n",
    "\n",
    "\n",
    "# 大纲\n",
    "- [ 1 - 包 ](#1)\n",
    "- [ 2 - ReLU 激活](#2)\n",
    "- [ 3 - Softmax 函数](#3)\n",
    "  - [ 练习 1](#ex01)\n",
    "- [ 4 - 神经网络](#4)\n",
    "  - [ 4.1 问题陈述](#4.1)\n",
    "  - [ 4.2 数据集](#4.2)\n",
    "  - [ 4.3 模型表示](#4.3)\n",
    "  - [ 4.4 Tensorflow 模型实现](#4.4)\n",
    "  - [ 4.5 Softmax 位置](#4.5)\n",
    "    - [ 练习 2](#ex02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - 包 \n",
    "\n",
    "首先，让我们运行下面的单元格，导入本次作业中需要的所有包。\n",
    "- [numpy](https://numpy.org/) 是 Python 科学计算的基础包。\n",
    "- [matplotlib](http://matplotlib.org) 是 Python 中流行的绘图库。\n",
    "- [tensorflow](https://www.tensorflow.org/) 是一个流行的机器学习平台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "from public_tests import * \n",
    "\n",
    "from autils import *\n",
    "from lab_utils_softmax import plt_softmax\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - ReLU 激活\n",
    "本周，引入了一种新的激活函数，即修正线性单元（ReLU）。 \n",
    "$$ a = max(0,z) \\quad\\quad\\text {# ReLU 函数} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aef69aa4f6146dfb4e92e7786bf102b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_act_trio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img align=\"right\" src=\"./images/C2_W2_ReLu.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "右侧讲座中的示例显示了 ReLU 的应用。在此示例中，派生的\"意识\"特征不是二元的，而是具有连续的值范围。Sigmoid 最适合开/关或二元情况。ReLU 提供连续的线性关系。此外，它有一个\"关闭\"范围，其中输出为零。     \n",
    "\"关闭\"特性使 ReLU 成为非线性激活。为什么需要这个？这使多个单元能够在不干扰的情况下对结果函数做出贡献。这在支持的可选实验中有更详细的检查。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Softmax 函数\n",
    "多分类神经网络生成 N 个输出。其中一个输出被选作预测答案。在输出层中，向量 $\\mathbf{z}$ 由一个线性函数生成，该函数应用于 softmax 函数。softmax 函数将 $\\mathbf{z}$ 转换为如下所述的概率分布。应用 softmax 后，每个输出将在 0 到 1 之间，并且输出的总和为 1，因此它们可以被解释为概率。较大的输入将对应于较大的输出概率。\n",
    "<center>  <img  src=\"./images/C2_W2_NNSoftmax.PNG\" width=\"600\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "softmax 函数可以写成：\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=0}^{N-1}{e^{z_k} }} \\tag{1}$$\n",
    "\n",
    "其中 $z = \\mathbf{w} \\cdot \\mathbf{x} + b$，N 是输出层中特征/类别的数量。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### 练习 1\n",
    "让我们创建一个 NumPy 实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED CELL: my_softmax\n",
    "\n",
    "def my_softmax(z):  \n",
    "    \"\"\" Softmax converts a vector of values to a probability distribution.\n",
    "    Args:\n",
    "      z (ndarray (N,))  : input data, N features\n",
    "    Returns:\n",
    "      a (ndarray (N,))  : softmax of z\n",
    "    \"\"\"    \n",
    "    ### START CODE HERE ### \n",
    "    ez = np.exp(z)\n",
    "    a = ez/np.sum(ez)\n",
    "    ### END CODE HERE ### \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_softmax(z):         [0.03 0.09 0.24 0.64]\n",
      "tensorflow softmax(z): [0.03 0.09 0.24 0.64]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "z = np.array([1., 2., 3., 4.])\n",
    "a = my_softmax(z)\n",
    "atf = tf.nn.softmax(z)\n",
    "print(f\"my_softmax(z):         {a}\")\n",
    "print(f\"tensorflow softmax(z): {atf}\")\n",
    "\n",
    "# BEGIN UNIT TEST  \n",
    "test_my_softmax(my_softmax)\n",
    "# END UNIT TEST  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    One implementation uses for loop to first build the denominator and then a second loop to calculate each output.\n",
    "    \n",
    "```python\n",
    "def my_softmax(z):  \n",
    "    N = len(z)\n",
    "    a =                     # initialize a to zeros \n",
    "    ez_sum =                # initialize sum to zero\n",
    "    for k in range(N):      # loop over number of outputs             \n",
    "        ez_sum +=           # sum exp(z[k]) to build the shared denominator      \n",
    "    for j in range(N):      # loop over number of outputs again                \n",
    "        a[j] =              # divide each the exp of each output by the denominator   \n",
    "    return(a)\n",
    "```\n",
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for code</b></font></summary>\n",
    "   \n",
    "```python\n",
    "def my_softmax(z):  \n",
    "    N = len(z)\n",
    "    a = np.zeros(N)\n",
    "    ez_sum = 0\n",
    "    for k in range(N):                \n",
    "        ez_sum += np.exp(z[k])       \n",
    "    for j in range(N):                \n",
    "        a[j] = np.exp(z[j])/ez_sum   \n",
    "    return(a)\n",
    "\n",
    "Or, a vector implementation:\n",
    "\n",
    "def my_softmax(z):  \n",
    "    ez = np.exp(z)              \n",
    "    a = ez/np.sum(ez)           \n",
    "    return(a)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "下面，改变 `z` 输入的值。特别注意分子中的指数如何放大值之间的微小差异。还要注意输出值的总和为一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5ea2c37bba41209c8e2157f63c2738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "plt_softmax(my_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Neural Networks\n",
    "\n",
    "In last weeks assignment, you implemented a neural network to do binary classification. This week you will extend that to multiclass classification. This will utilize the softmax activation.\n",
    "\n",
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### 4.1 Problem Statement\n",
    "\n",
    "In this exercise, you will use a neural network to recognize ten handwritten digits, 0-9. This is a multiclass classification task where one of n choices is selected. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. \n",
    "\n",
    "\n",
    "<a name=\"4.2\"></a>\n",
    "### 4.2 Dataset\n",
    "\n",
    "You will start by loading the dataset for this task. \n",
    "- The `load_data()` function shown below loads the data into variables `X` and `y`\n",
    "\n",
    "\n",
    "- The data set contains 5000 training examples of handwritten digits $^1$.  \n",
    "\n",
    "    - Each training example is a 20-pixel x 20-pixel grayscale image of the digit. \n",
    "        - Each pixel is represented by a floating-point number indicating the grayscale intensity at that location. \n",
    "        - The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. \n",
    "        - Each training examples becomes a single row in our data matrix `X`. \n",
    "        - This gives us a 5000 x 400 matrix `X` where every row is a training example of a handwritten digit image.\n",
    "\n",
    "$$X = \n",
    "\\left(\\begin{array}{cc} \n",
    "--- (x^{(1)}) --- \\\\\n",
    "--- (x^{(2)}) --- \\\\\n",
    "\\vdots \\\\ \n",
    "--- (x^{(m)}) --- \n",
    "\\end{array}\\right)$$ \n",
    "\n",
    "- The second part of the training set is a 5000 x 1 dimensional vector `y` that contains labels for the training set\n",
    "    - `y = 0` if the image is of the digit `0`, `y = 4` if the image is of the digit `4` and so on.\n",
    "\n",
    "$^1$<sub> This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.2.1 查看变量\n",
    "让我们更熟悉您的数据集。  \n",
    "- 一个好的起点是打印出每个变量并查看它包含的内容。\n",
    "\n",
    "下面的代码打印变量 `X` 和 `y` 中的第一个元素。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('The first element of X is: ', X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of y is:  0\n",
      "The last element of y is:  9\n"
     ]
    }
   ],
   "source": [
    "print ('The first element of y is: ', y[0,0])\n",
    "print ('The last element of y is: ', y[-1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.2.2 检查变量的维度\n",
    "\n",
    "另一种熟悉数据的方法是查看其维度。请打印 `X` 和 `y` 的形状，并查看数据集中有多少训练样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (5000, 400)\n",
      "The shape of y is: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X is: ' + str(X.shape))\n",
    "print ('The shape of y is: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.2.3 可视化数据\n",
    "\n",
    "您将从可视化训练集的子集开始。 \n",
    "- 在下面的单元格中，代码从 `X` 中随机选择 64 行，将每一行映射回 20 像素 x 20 像素的灰度图像，并一起显示这些图像。 \n",
    "- 每个图像的标签显示在图像上方 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66746b327ff4eb4a92df796eff793bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# You do not need to modify anything in this cell\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "fig, axes = plt.subplots(8,8, figsize=(5,5))\n",
    "fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]\n",
    "\n",
    "#fig.tight_layout(pad=0.5)\n",
    "widgvis(fig)\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    # Select random indices\n",
    "    random_index = np.random.randint(m)\n",
    "    \n",
    "    # Select rows corresponding to the random indices and\n",
    "    # reshape the image\n",
    "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(X_random_reshaped, cmap='gray')\n",
    "    \n",
    "    # Display the label above the image\n",
    "    ax.set_title(y[random_index,0])\n",
    "    ax.set_axis_off()\n",
    "    fig.suptitle(\"Label, image\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### 4.3 模型表示\n",
    "\n",
    "您将在本次作业中使用的神经网络如下图所示。 \n",
    "- 这有两个带 ReLU 激活的密集层，后跟一个带线性激活的输出层。 \n",
    "    - 回想一下，我们的输入是数字图像的像素值。\n",
    "    - 由于图像大小为 $20\\times20$，这给了我们 $400$ 个输入  \n",
    "    \n",
    "<img src=\"images/C2_W2_Assigment_NN.png\" width=\"600\" height=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 参数的维度是为具有第 1 层 $25$ 个单元、第 2 层 $15$ 个单元和第 3 层 $10$ 个输出单元（每个数字一个）的神经网络调整大小的。\n",
    "\n",
    "    - 回想一下，这些参数的维度确定如下：\n",
    "        - 如果网络在一层中有 $s_{in}$ 个单元，在下一层中有 $s_{out}$ 个单元，那么 \n",
    "            - $W$ 的维度将是 $s_{in} \\times s_{out}$。\n",
    "            - $b$ 将是一个具有 $s_{out}$ 个元素的向量\n",
    "  \n",
    "    - 因此，`W` 和 `b` 的形状是 \n",
    "        - layer1: `W1` 的形状是 (400, 25)，`b1` 的形状是 (25,)\n",
    "        - layer2: `W2` 的形状是 (25, 15)，`b2` 的形状是: (15,)\n",
    "        - layer3: `W3` 的形状是 (15, 10)，`b3` 的形状是: (10,)\n",
    ">**注意：** 偏置向量 `b` 可以表示为 1-D (n,) 或 2-D (n,1) 数组。Tensorflow 使用 1-D 表示，本实验将保持该约定： \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### 4.4 Tensorflow 模型实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tensorflow 模型是逐层构建的。层的输入维度（上面的 $s_{in}$）是为您计算的。您指定层的*输出维度*，这决定了下一层的输入维度。第一层的输入维度是从下面 `model.fit` 语句中指定的输入数据的大小推导出来的。 \n",
    ">**注意：** 也可以添加一个指定第一层输入维度的输入层。例如：  \n",
    "`tf.keras.Input(shape=(400,)),    #指定输入形状`  \n",
    "我们将在这里包含它来说明一些模型大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"4.5\"></a>\n",
    "### 4.5 Softmax 位置\n",
    "如讲座和可选的 softmax 实验所述，如果在训练期间将 softmax 与损失函数分组而不是与输出层分组，则数值稳定性会提高。这在*构建*模型和*使用*模型时都有影响。  \n",
    "构建：  \n",
    "* 最终的 Dense 层应使用 'linear' 激活。这实际上是没有激活。 \n",
    "* `model.compile` 语句将通过包含 `from_logits=True` 来指示这一点。\n",
    "`loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) `  \n",
    "* 这不会影响目标的形式。在 SparseCategorialCrossentropy 的情况下，目标是期望的数字，0-9。\n",
    "\n",
    "使用模型：\n",
    "* 输出不是概率。如果需要输出概率，请应用 softmax 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"ex02\"></a>\n",
    "### 练习 2\n",
    "\n",
    "下面，使用 Keras [Sequential 模型](https://keras.io/guides/sequential_model/)和带 ReLU 激活的 [Dense 层](https://keras.io/api/layers/core_layers/dense/)来构建上面描述的三层网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED CELL: Sequential model\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [               \n",
    "        ### START CODE HERE ### \n",
    "        tf.keras.layers.InputLayer((400,)),\n",
    "        tf.keras.layers.Dense(25, activation=\"relu\", name=\"L1\"),\n",
    "        tf.keras.layers.Dense(15, activation=\"relu\", name=\"L2\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"linear\", name=\"L3\")\n",
    "        ### END CODE HERE ### \n",
    "    ], name = \"my_model\" \n",
    ")\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " L1 (Dense)                  (None, 25)                10025     \n",
      "                                                                 \n",
      " L2 (Dense)                  (None, 15)                390       \n",
      "                                                                 \n",
      " L3 (Dense)                  (None, 10)                160       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,575\n",
      "Trainable params: 10,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Expected Output (Click to expand)</b></font></summary>\n",
    "The `model.summary()` function displays a useful summary of the model. Note, the names of the layers may vary as they are auto-generated unless the name is specified.    \n",
    "    \n",
    "```\n",
    "Model: \"my_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "L1 (Dense)                   (None, 25)                10025     \n",
    "_________________________________________________________________\n",
    "L2 (Dense)                   (None, 15)                390       \n",
    "_________________________________________________________________\n",
    "L3 (Dense)                   (None, 10)                160       \n",
    "=================================================================\n",
    "Total params: 10,575\n",
    "Trainable params: 10,575\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "```python\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential(\n",
    "    [               \n",
    "        ### START CODE HERE ### \n",
    "        tf.keras.Input(shape=(400,)),     # @REPLACE \n",
    "        Dense(25, activation='relu', name = \"L1\"), # @REPLACE \n",
    "        Dense(15, activation='relu',  name = \"L2\"), # @REPLACE  \n",
    "        Dense(10, activation='linear', name = \"L3\"),  # @REPLACE \n",
    "        ### END CODE HERE ### \n",
    "    ], name = \"my_model\" \n",
    ")\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST     \n",
    "test_model(model, 10, 400)\n",
    "# END UNIT TEST     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "摘要中显示的参数计数对应于权重和偏置数组中的元素数量，如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "让我们进一步检查权重，以验证 tensorflow 产生的维度与我们上面计算的相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[layer1, layer2, layer3] = model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape = (400, 25), b1 shape = (25,)\n",
      "W2 shape = (25, 15), b2 shape = (15,)\n",
      "W3 shape = (15, 10), b3 shape = (10,)\n"
     ]
    }
   ],
   "source": [
    "#### 检查权重形状\n",
    "W1,b1 = layer1.get_weights()\n",
    "W2,b2 = layer2.get_weights()\n",
    "W3,b3 = layer3.get_weights()\n",
    "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
    "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
    "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected Output**\n",
    "```\n",
    "W1 shape = (400, 25), b1 shape = (25,)  \n",
    "W2 shape = (25, 15), b2 shape = (15,)  \n",
    "W3 shape = (15, 10), b3 shape = (10,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "以下代码：\n",
    "* 定义一个损失函数 `SparseCategoricalCrossentropy`，并通过添加 `from_logits=True` 来指示 softmax 应包含在损失计算中）\n",
    "* 定义一个优化器。一个流行的选择是自适应矩（Adam），这在讲座中已描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 1.7094\n",
      "Epoch 2/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.7480\n",
      "Epoch 3/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.4428\n",
      "Epoch 4/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3463\n",
      "Epoch 5/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2977\n",
      "Epoch 6/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2630\n",
      "Epoch 7/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2361\n",
      "Epoch 8/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2131\n",
      "Epoch 9/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2004\n",
      "Epoch 10/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1805\n",
      "Epoch 11/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1692\n",
      "Epoch 12/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1580\n",
      "Epoch 13/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1507\n",
      "Epoch 14/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1396\n",
      "Epoch 15/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1289\n",
      "Epoch 16/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1255\n",
      "Epoch 17/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1154\n",
      "Epoch 18/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1102\n",
      "Epoch 19/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1016\n",
      "Epoch 20/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0970\n",
      "Epoch 21/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0926\n",
      "Epoch 22/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 23/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 24/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 25/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0755\n",
      "Epoch 26/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0713\n",
      "Epoch 27/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0701\n",
      "Epoch 28/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0617\n",
      "Epoch 29/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0578\n",
      "Epoch 30/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0550\n",
      "Epoch 31/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0511\n",
      "Epoch 32/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0499\n",
      "Epoch 33/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0462\n",
      "Epoch 34/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0437\n",
      "Epoch 35/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0422\n",
      "Epoch 36/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0396\n",
      "Epoch 37/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0366\n",
      "Epoch 38/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0344\n",
      "Epoch 39/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 40/40\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0294\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X,y,\n",
    "    epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Epochs 和 batches\n",
    "在上面的 `compile` 语句中，`epochs` 的数量设置为 100。这指定整个数据集应在训练期间应用 100 次。在训练期间，您会看到描述训练进度的输出，如下所示：\n",
    "```\n",
    "Epoch 1/100\n",
    "157/157 [==============================] - 0s 1ms/step - loss: 2.2770\n",
    "```\n",
    "第一行 `Epoch 1/100` 描述模型当前正在运行的 epoch。为了提高效率，训练数据集被分成\"批次\"。Tensorflow 中批次的默认大小为 32。我们的数据集中有 5000 个样本，大约 157 个批次。第二行上的符号 `157/157 [====` 描述已执行了哪个批次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Loss（代价）\n",
    "在课程 1 中，我们学会了通过监控代价来跟踪梯度下降的进度。理想情况下，代价会随着算法迭代次数的增加而减少。Tensorflow 将代价称为 `loss`。上面，您看到在 `model.fit` 执行时每个 epoch 显示的损失。[.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model) 方法返回包括损失在内的各种指标。这在上面的 `history` 变量中捕获。这可用于在如下所示的图中检查损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85765547adaa44e197f57464c49a0ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_tf(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 预测 \n",
    "要进行预测，请使用 Keras `predict`。下面，X[1015] 包含一个数字 2 的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0cfe835bd14b1fb3b8a9fa09b3167e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicting a Two: \n",
      "[[ -7.99  -2.23   0.77  -2.41 -11.66 -11.15  -9.53  -3.36  -4.42  -7.17]]\n",
      " Largest Prediction index: 2\n"
     ]
    }
   ],
   "source": [
    "image_of_two = X[1015]\n",
    "display_digit(image_of_two)\n",
    "\n",
    "prediction = model.predict(image_of_two.reshape(1,400))  # prediction\n",
    "\n",
    "print(f\" predicting a Two: \\n{prediction}\")\n",
    "print(f\" Largest Prediction index: {np.argmax(prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "最大输出是 prediction[2]，表示预测的数字是 '2'。如果问题只需要选择，这就足够了。使用 NumPy [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) 来选择它。如果问题需要概率，则需要 softmax："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicting a Two. Probability vector: \n",
      "[[1.42e-04 4.49e-02 8.98e-01 3.76e-02 3.61e-06 5.97e-06 3.03e-05 1.44e-02\n",
      "  5.03e-03 3.22e-04]]\n",
      "Total of predictions: 1.000\n"
     ]
    }
   ],
   "source": [
    "prediction_p = tf.nn.softmax(prediction)\n",
    "\n",
    "print(f\" predicting a Two. Probability vector: \\n{prediction_p}\")\n",
    "print(f\"Total of predictions: {np.sum(prediction_p):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "要返回表示预测目标的整数，您需要最大概率的索引。这可以通过 Numpy [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) 函数来完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.argmax(prediction_p): 2\n"
     ]
    }
   ],
   "source": [
    "yhat = np.argmax(prediction_p)\n",
    "\n",
    "print(f\"np.argmax(prediction_p): {yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "让我们比较 64 个随机数字样本的预测与标签。这需要一些时间才能运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4190acc8b7cf4617856f2ce6f839880f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# You do not need to modify anything in this cell\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "fig, axes = plt.subplots(8,8, figsize=(5,5))\n",
    "fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]\n",
    "widgvis(fig)\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    # Select random indices\n",
    "    random_index = np.random.randint(m)\n",
    "    \n",
    "    # Select rows corresponding to the random indices and\n",
    "    # reshape the image\n",
    "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(X_random_reshaped, cmap='gray')\n",
    "    \n",
    "    # Predict using the Neural Network\n",
    "    prediction = model.predict(X[random_index].reshape(1,400))\n",
    "    prediction_p = tf.nn.softmax(prediction)\n",
    "    yhat = np.argmax(prediction_p)\n",
    "    \n",
    "    # Display the label above the image\n",
    "    ax.set_title(f\"{y[random_index,0]},{yhat}\",fontsize=10)\n",
    "    ax.set_axis_off()\n",
    "fig.suptitle(\"Label, yhat\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "让我们看看一些错误。 \n",
    ">注意：增加训练 epoch 的数量可以消除此数据集上的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4df6ce60e9470a9c5b1fe8bef3df02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 errors out of 5000 images\n"
     ]
    }
   ],
   "source": [
    "print( f\"{display_errors(model,X,y)} errors out of {len(X)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 恭喜！\n",
    "您已经成功构建并利用神经网络进行多分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "89367"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
