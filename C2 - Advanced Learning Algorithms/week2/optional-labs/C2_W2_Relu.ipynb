{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可选实验 - ReLU 激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from autils import plt_act_trio\n",
    "from lab_utils_relu import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - ReLU 激活\n",
    "本周，引入了一种新的激活函数，即修正线性单元（ReLU）。 \n",
    "$$ a = max(0,z) \\quad\\quad\\text {# ReLU 函数} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8631645d39248c7bba85e5608139136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_act_trio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"./images/C2_W2_ReLu.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "右侧讲座中的示例显示了 ReLU 的应用。在此示例中，派生的\"意识\"特征不是二元的，而是具有连续的值范围。Sigmoid 最适合开/关或二元情况。ReLU 提供连续的线性关系。此外，它有一个\"关闭\"范围，其中输出为零。     \n",
    "\"关闭\"特性使 ReLU 成为非线性激活。为什么需要这个？让我们在下面检查一下。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么需要非线性激活？  \n",
    "<img align=\"left\" src=\"./images/C2_W2_ReLU_Graph.png\"     style=\" width:250px; padding: 10px 20px; \" > 显示的函数由线性片段组成（分段线性）。斜率在线性部分保持一致，然后在过渡点突然改变。在过渡点，添加一个新的线性函数，当添加到现有函数时，将产生新的斜率。新函数在过渡点添加，但在该点之前不会对输出做出贡献。非线性激活函数负责在过渡点之前（有时之后）禁用输入。下面的练习提供了一个更具体的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该练习将在回归问题中使用下面的网络，您必须对分段线性目标进行建模：\n",
    "<img align=\"center\" src=\"./images/C2_W2_ReLU_Network.png\"     style=\" width:650px; padding: 10px 20px; \">  \n",
    "网络在第一层有 3 个单元。每个单元都需要形成目标。单元 0 是预编程的并固定为映射第一段。您将修改单元 1 和 2 中的权重和偏置以建模第二段和第三段。输出单元也是固定的，只是对第一层的输出求和。  \n",
    "\n",
    "使用下面的滑块，修改权重和偏置以匹配目标。 \n",
    "提示：从 `w1` 和 `b1` 开始，将 `w2` 和 `b2` 保持为零，直到您匹配第二段。点击而不是滑动更快。如果您遇到困难，不要担心，下面的文本将更详细地描述这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89e83fedc19457bb6a7267d3ba2f530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt_relu_ex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "本练习的目标是理解 ReLU 的非线性行为如何提供在需要之前关闭函数所需的能力。让我们看看这在这个示例中是如何工作的。\n",
    "<img align=\"right\" src=\"./images/C2_W2_ReLU_Plot.png\"     style=\" width:600px; padding: 10px 20px; \"> \n",
    "右侧的图包含第一层中单元的输出。   \n",
    "从顶部开始，单元 0 负责标记为 1 的第一段。显示了线性函数 $z$ 和 ReLU 之后的函数 $a$。您可以看到 ReLU 在区间 [0,1] 之后切断了函数。这很重要，因为它防止单元 0 干扰后续段。 \n",
    "\n",
    "单元 1 负责第二段。在这里，ReLU 使该单元保持安静，直到 x 大于 1。由于第一个单元没有贡献，单元 1 的斜率 $w^{[1]}_1$ 就是目标线的斜率。必须调整偏置以保持输出为负，直到 x 达到 1。请注意，单元 1 的贡献也延伸到第三段。\n",
    "\n",
    "单元 2 负责第三段。ReLU 再次将输出置零，直到 x 达到正确的值。单元的斜率 $w^{[1]}_2$ 必须设置，使得单元 1 和 2 的总和具有所需的斜率。再次调整偏置以保持输出为负，直到 x 达到 2。 \n",
    "\n",
    "ReLU 激活的\"关闭\"或禁用功能使模型能够将线性段拼接在一起，以建模复杂的非线性函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恭喜！\n",
    "您现在更熟悉 ReLU 及其非线性行为的重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
