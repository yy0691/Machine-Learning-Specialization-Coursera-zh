{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实践实验：决策树\n",
    "\n",
    "在本练习中，您将从零开始实现决策树，并将其应用于分类蘑菇是否可食用的任务。\n",
    "\n",
    "# 大纲\n",
    "- [ 1 - 包 ](#1)\n",
    "- [ 2 - 问题陈述](#2)\n",
    "- [ 3 - 数据集](#3)\n",
    "  - [ 3.1 独热编码数据集](#3.1)\n",
    "- [ 4 - 决策树回顾](#4)\n",
    "  - [ 4.1 计算熵](#4.1)\n",
    "    - [ 练习 1](#ex01)\n",
    "  - [ 4.2 分割数据集](#4.2)\n",
    "    - [ 练习 2](#ex02)\n",
    "  - [ 4.3 计算信息增益](#4.3)\n",
    "    - [ 练习 3](#ex03)\n",
    "  - [ 4.4 获取最佳分割](#4.4)\n",
    "    - [ 练习 4](#ex04)\n",
    "- [ 5 - 构建树](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - 包 \n",
    "\n",
    "首先，让我们运行下面的单元格，导入本次作业中需要的所有包。\n",
    "- [numpy](https://www.numpy.org) 是 Python 中处理矩阵的基础包。\n",
    "- [matplotlib](https://matplotlib.org) 是 Python 中用于绘制图形的著名库。\n",
    "- ``utils.py`` 包含本次作业的辅助函数。您不需要修改此文件中的代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - 问题陈述\n",
    "\n",
    "假设您正在创办一家种植和销售野生蘑菇的公司。 \n",
    "- 由于并非所有蘑菇都可食用，您希望能够根据给定蘑菇的物理属性来判断它是否可食用或有毒\n",
    "- 您有一些可用于此任务的现有数据。 \n",
    "\n",
    "您可以使用数据来帮助识别哪些蘑菇可以安全销售吗？ \n",
    "\n",
    "注意：使用的数据集仅用于说明目的。它不旨在作为识别可食用蘑菇的指南。\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"3\"></a>\n",
    "## 3 - 数据集\n",
    "\n",
    "您将首先加载此任务的数据集。您收集的数据集如下：\n",
    "\n",
    "| 菌盖颜色 | 菌柄形状 | 单独生长 | 可食用 |\n",
    "|:---------:|:-----------:|:--------:|:------:|\n",
    "|   棕色   |   渐细  |    是   |    1   |\n",
    "|   棕色   |  扩大  |    是   |    1   |\n",
    "|   棕色   |  扩大  |    否    |    0   |\n",
    "|   棕色   |  扩大  |    否    |    0   |\n",
    "|   棕色   |   渐细  |    是   |    1   |\n",
    "|    红色    |   渐细  |    是   |    0   |\n",
    "|    红色    |  扩大  |    否    |    0   |\n",
    "|   棕色   |  扩大  |    是   |    1   |\n",
    "|    红色    |   渐细  |    否    |    1   |\n",
    "|   棕色   |  扩大  |    否    |    0   |\n",
    "\n",
    "\n",
    "-  您有 10 个蘑菇样本。对于每个样本，您有\n",
    "    - 三个特征\n",
    "        - 菌盖颜色（`棕色` 或 `红色`），\n",
    "        - 菌柄形状（`渐细` 或 `扩大`），以及\n",
    "        - 单独生长（`是` 或 `否`）\n",
    "    - 标签\n",
    "        - 可食用（`1` 表示是或 `0` 表示有毒）\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 独热编码数据集\n",
    "为了便于实现，我们对特征进行了独热编码（将它们转换为 0 或 1 值的特征）\n",
    "\n",
    "| 棕色菌盖 | 渐细菌柄形状 | 单独生长 | 可食用 |\n",
    "|:---------:|:--------------------:|:--------:|:------:|\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     0     |           1          |     1    |    0   |\n",
    "|     0     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     0     |           1          |     0    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "\n",
    "因此，\n",
    "- `X_train` 包含每个样本的三个特征 \n",
    "    - 棕色（值为 `1` 表示\"棕色\"菌盖颜色，`0` 表示\"红色\"菌盖颜色）\n",
    "    - 渐细形状（值为 `1` 表示\"渐细菌柄形状\"，`0` 表示\"扩大\"菌柄形状）\n",
    "    - 单独生长（值为 `1` 表示\"是\"，`0` 表示\"否\"）\n",
    "\n",
    "- `y_train` 是蘑菇是否可食用 \n",
    "    - `y = 1` 表示可食用\n",
    "    - `y = 0` 表示有毒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看变量\n",
    "让我们更熟悉您的数据集。  \n",
    "- 一个好的起点是打印出每个变量并查看它包含的内容。\n",
    "\n",
    "下面的代码打印 `X_train` 的前几个元素和变量的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few elements of X_train:\n",
      " [[1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "Type of X_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First few elements of X_train:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们对 `y_train` 做同样的事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few elements of y_train: [1 1 0 0 1]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First few elements of y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查变量的维度\n",
    "\n",
    "另一种熟悉数据的有用方法是查看其维度。\n",
    "\n",
    "请打印 `X_train` 和 `y_train` 的形状，并查看数据集中有多少个训练样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (10, 3)\n",
      "The shape of y_train is:  (10,)\n",
      "Number of training examples (m): 10\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - 决策树回顾\n",
    "\n",
    "在本实践实验中，您将基于提供的数据集构建决策树。\n",
    "\n",
    "- 回想一下，构建决策树的步骤如下：\n",
    "    - 从根节点的所有样本开始\n",
    "    - 计算在所有可能特征上分割的信息增益，并选择信息增益最高的特征\n",
    "    - 根据所选特征分割数据集，并创建树的左右分支\n",
    "    - 继续重复分割过程，直到满足停止条件\n",
    "  \n",
    "  \n",
    "- 在本实验中，您将实现以下函数，这些函数将让您使用信息增益最高的特征将节点分割为左右分支\n",
    "    - 计算节点的熵 \n",
    "    - 根据给定特征将节点处的数据集分割为左右分支\n",
    "    - 计算在给定特征上分割的信息增益\n",
    "    - 选择使信息增益最大化的特征\n",
    "    \n",
    "- 然后，我们将使用您实现的辅助函数，通过重复分割过程来构建决策树，直到满足停止条件 \n",
    "    - 对于本实验，我们选择的停止条件是设置最大深度为 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### 4.1 计算熵\n",
    "\n",
    "首先，您将编写一个名为 `compute_entropy` 的辅助函数，该函数计算节点的熵（不纯度的度量）。 \n",
    "- 该函数接受一个 numpy 数组（`y`），该数组指示该节点中的样本是否可食用（`1`）或有毒（`0`） \n",
    "\n",
    "完成下面的 `compute_entropy()` 函数以：\n",
    "* 计算 $p_1$，即可食用样本的比例（即在 `y` 中值为 `1` 的样本）\n",
    "* 然后熵计算为 \n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "* 注意 \n",
    "    * 对数以 $2$ 为底计算\n",
    "    * 为了实现目的，$0\\text{log}_2(0) = 0$。也就是说，如果 `p_1 = 0` 或 `p_1 = 1`，将熵设置为 `0`\n",
    "    * 确保检查节点处的数据不为空（即 `len(y) != 0`）。如果为空，返回 `0`\n",
    "    \n",
    "<a name=\"ex01\"></a>\n",
    "### 练习 1\n",
    "\n",
    "请使用前面的说明完成 `compute_entropy()` 函数。\n",
    "    \n",
    "如果您遇到困难，可以查看下面单元格后提供的提示以帮助您实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: compute_entropy\n",
    "\n",
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for \n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           edible (`1`) or poisonous (`0`)\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "        \n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly\n",
    "    entropy = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    if len(y) != 0:\n",
    "        p1 = p1 = len(y[y == 1]) / len(y) \n",
    "     # For p1 = 0 and 1, set the entropy to 0 (to handle 0log0)\n",
    "        if p1 != 0 and p1 != 1:\n",
    "             entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "        else:\n",
    "             entropy = 0\n",
    "    ### END CODE HERE ###        \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>点击查看提示</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * 要计算 `p1`\n",
    "       * 您可以将 `y` 中值为 `1` 的样本子集获取为 `y[y == 1]`\n",
    "       * 您可以使用 `len(y)` 来获取 `y` 中的样本数量\n",
    "   * 要计算 `entropy`\n",
    "       * <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log2.html\">np.log2</a> 允许您计算 numpy 数组的以 2 为底的对数\n",
    "       * 如果 `p1` 的值为 0 或 1，请确保将熵设置为 `0` \n",
    "     \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> 点击查看更多提示</b></font></summary>\n",
    "        \n",
    "    * 以下是您如何构建此函数的整体实现\n",
    "    ```python \n",
    "    def compute_entropy(y):\n",
    "        \n",
    "        # 您需要正确返回以下变量\n",
    "        entropy = 0.\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        if len(y) != 0:\n",
    "            # 您的代码：计算可食用样本的比例（即在 y 中值为 1 的样本）\n",
    "            p1 =\n",
    "\n",
    "            # 对于 p1 = 0 和 1，将熵设置为 0（以处理 0log0）\n",
    "            if p1 != 0 and p1 != 1:\n",
    "                # 您的代码：使用上面提供的公式计算熵\n",
    "                entropy = \n",
    "            else:\n",
    "                entropy = 0. \n",
    "        ### END CODE HERE ###        \n",
    "\n",
    "        return entropy\n",
    "    ```\n",
    "    \n",
    "    如果您仍然遇到困难，可以查看下面提供的提示，以了解如何计算 `p1` 和 `entropy`。\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>计算 p1 的提示</b></font></summary>\n",
    "           &emsp; &emsp; 您可以将 p1 计算为 <code>p1 = len(y[y == 1]) / len(y) </code>\n",
    "    </details>\n",
    "\n",
    "     <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>计算 entropy 的提示</b></font></summary>\n",
    "          &emsp; &emsp; 您可以将 entropy 计算为 <code>entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)</code>\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以通过运行以下测试代码来检查您的实现是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root node:  1.0\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Compute entropy at the root node (i.e. with all examples)\n",
    "# Since we have 5 edible and 5 non-edible mushrooms, the entropy should be 1\"\n",
    "\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train)) \n",
    "\n",
    "# UNIT TESTS\n",
    "compute_entropy_test(compute_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**：\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>根节点处的熵：<b> 1.0 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### 4.2 分割数据集\n",
    "\n",
    "接下来，您将编写一个名为 `split_dataset` 的辅助函数，该函数接受节点处的数据和要分割的特征，并将其分割为左右分支。稍后在实验中，您将实现代码来计算分割的好坏。\n",
    "\n",
    "- 该函数接受训练数据、该节点处数据点的索引列表以及要分割的特征。 \n",
    "- 它分割数据并返回左右分支处的索引子集。\n",
    "- 例如，假设我们从根节点开始（所以 `node_indices = [0,1,2,3,4,5,6,7,8,9]`），我们选择在特征 `0` 上分割，即样本是否有棕色菌盖。\n",
    "    - 然后函数的输出是 `left_indices = [0,1,2,3,4,7,9]` 和 `right_indices = [5,6,8]`\n",
    "    \n",
    "| 索引 | 棕色菌盖 | 渐细菌柄形状 | 单独生长 | 可食用 |\n",
    "|:-----:|:---------:|:--------------------:|:--------:|:------:|\n",
    "|   0   |     1     |           1          |     1    |    1   |\n",
    "|   1   |     1     |           0          |     1    |    1   |\n",
    "|   2   |     1     |           0          |     0    |    0   |\n",
    "|   3   |     1     |           0          |     0    |    0   |\n",
    "|   4   |     1     |           1          |     1    |    1   |\n",
    "|   5   |     0     |           1          |     1    |    0   |\n",
    "|   6   |     0     |           0          |     0    |    0   |\n",
    "|   7   |     1     |           0          |     1    |    1   |\n",
    "|   8   |     0     |           1          |     0    |    1   |\n",
    "|   9   |     1     |           0          |     0    |    0   |\n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### 练习 2\n",
    "\n",
    "请完成下面显示的 `split_dataset()` 函数\n",
    "\n",
    "- 对于 `node_indices` 中的每个索引\n",
    "    - 如果 `X` 在该索引处该特征的值是 `1`，将索引添加到 `left_indices`\n",
    "    - 如果 `X` 在该索引处该特征的值是 `0`，将索引添加到 `right_indices`\n",
    "\n",
    "如果您遇到困难，可以查看下面单元格后提供的提示以帮助您实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: split_dataset\n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):  List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list): Indices with feature value == 1\n",
    "        right_indices (list): Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in node_indices:   \n",
    "        if X[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>点击查看提示</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * 以下是您如何构建此函数的整体实现\n",
    "    ```python \n",
    "    def split_dataset(X, node_indices, feature):\n",
    "    \n",
    "        # 您需要正确返回以下变量\n",
    "        left_indices = []\n",
    "        right_indices = []\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # 遍历该节点处的样本索引\n",
    "        for i in node_indices:   \n",
    "            if # 您的代码：检查 X 在该索引处该特征的值是否为 1\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return left_indices, right_indices\n",
    "    ```\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> 点击查看更多提示</b></font></summary>\n",
    "        \n",
    "    条件是 <code> if X[i][feature] == 1:</code>。\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们使用下面的代码块检查您的实现。让我们尝试在根节点处分割数据集，该节点包含特征 0（棕色菌盖）的所有样本，正如我们上面讨论的那样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "Right indices:  [5, 6, 8]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Feel free to play around with these variables\n",
    "# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# UNIT TESTS    \n",
    "split_dataset_test(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**：\n",
    "```\n",
    "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
    "Right indices:  [5, 6, 8]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### 4.3 计算信息增益\n",
    "\n",
    "接下来，您将编写一个名为 `information_gain` 的函数，该函数接受训练数据、节点处的索引和要分割的特征，并返回分割的信息增益。\n",
    "\n",
    "<a name=\"ex03\"></a>\n",
    "### 练习 3\n",
    "\n",
    "请完成下面显示的 `compute_information_gain()` 函数以计算\n",
    "\n",
    "$$\\text{信息增益} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$\n",
    "\n",
    "其中 \n",
    "- $H(p_1^\\text{node})$ 是节点处的熵 \n",
    "- $H(p_1^\\text{left})$ 和 $H(p_1^\\text{right})$ 是分割产生的左右分支处的熵\n",
    "- $w^{\\text{left}}$ 和 $w^{\\text{right}}$ 分别是左右分支处的样本比例\n",
    "\n",
    "注意：\n",
    "- 您可以使用上面实现的 `compute_entropy()` 函数来计算熵\n",
    "- 我们提供了一些起始代码，使用您上面实现的 `split_dataset()` 函数来分割数据集 \n",
    "\n",
    "如果您遇到困难，可以查看下面单元格后提供的提示以帮助您实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: compute_information_gain\n",
    "\n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "   \n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    information_gain = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    \n",
    "    # Weights \n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    \n",
    "    #Weighted entropy\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    \n",
    "    #Information gain \n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    ### END CODE HERE ###  \n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>点击查看提示</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * 以下是您如何构建此函数的整体实现\n",
    "    ```python \n",
    "    def compute_information_gain(X, y, node_indices, feature):\n",
    "        # 分割数据集\n",
    "        left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "\n",
    "        # 一些有用的变量\n",
    "        X_node, y_node = X[node_indices], y[node_indices]\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "        # 您需要正确返回以下变量\n",
    "        information_gain = 0\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # 您的代码：使用 compute_entropy() 计算节点处的熵\n",
    "        node_entropy = \n",
    "        # 您的代码：计算左分支处的熵\n",
    "        left_entropy = \n",
    "        # 您的代码：计算右分支处的熵\n",
    "        right_entropy = \n",
    "\n",
    "        # 您的代码：计算左分支处的样本比例\n",
    "        w_left = \n",
    "        \n",
    "        # 您的代码：计算右分支处的样本比例\n",
    "        w_right = \n",
    "\n",
    "        # 您的代码：使用 w_left、w_right、left_entropy 和 right_entropy 计算分割的加权熵\n",
    "        weighted_entropy = \n",
    "\n",
    "        # 您的代码：将信息增益计算为节点处的熵\n",
    "        # 减去加权熵\n",
    "        information_gain = \n",
    "        ### END CODE HERE ###  \n",
    "\n",
    "        return information_gain\n",
    "    ```\n",
    "    如果您仍然遇到困难，请查看下面的提示。\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> 计算熵的提示</b></font></summary>\n",
    "        \n",
    "    <code>node_entropy = compute_entropy(y_node)</code><br>\n",
    "    <code>left_entropy = compute_entropy(y_left)</code><br>\n",
    "    <code>right_entropy = compute_entropy(y_right)</code>\n",
    "        \n",
    "    </details>\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>计算 w_left 和 w_right 的提示</b></font></summary>\n",
    "           <code>w_left = len(X_left) / len(X_node)</code><br>\n",
    "           <code>w_right = len(X_right) / len(X_node)</code>\n",
    "    </details>\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>计算 weighted_entropy 的提示</b></font></summary>\n",
    "           <code>weighted_entropy = w_left * left_entropy + w_right * right_entropy</code>\n",
    "    </details>\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>计算 information_gain 的提示</b></font></summary>\n",
    "           <code> information_gain = node_entropy - weighted_entropy</code>\n",
    "    </details>\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您现在可以使用下面的单元格检查您的实现，并计算在每个特征上分割的信息增益"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
      "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
      "Information Gain from splitting the root on solitary:  0.2780719051126377\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "    \n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_information_gain_test(compute_information_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**：\n",
    "```\n",
    "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
    "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
    "Information Gain from splitting the root on solitary:  0.2780719051126377\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在根节点处分割\"单独生长\"（特征 = 2）给出最大信息增益。因此，它是在根节点处分割的最佳特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### 4.4 获取最佳分割\n",
    "现在让我们编写一个函数，通过计算每个特征的信息增益（如上面所做的那样）来获取最佳分割特征，并返回给出最大信息增益的特征\n",
    "\n",
    "<a name=\"ex04\"></a>\n",
    "### 练习 4\n",
    "请完成下面显示的 `get_best_split()` 函数。\n",
    "- 该函数接受训练数据以及该节点处数据点的索引\n",
    "- 函数的输出是给出最大信息增益的特征 \n",
    "    - 您可以使用 `compute_information_gain()` 函数遍历特征并计算每个特征的信息增益\n",
    "如果您遇到困难，可以查看下面单元格后提供的提示以帮助您实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: get_best_split\n",
    "\n",
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    max_info_gain=0\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "                \n",
    "        \n",
    "    ### END CODE HERE ##    \n",
    "       \n",
    "    return best_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>点击查看提示</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * 以下是您如何构建此函数的整体实现\n",
    "    \n",
    "    ```python \n",
    "    def get_best_split(X, y, node_indices):   \n",
    "\n",
    "        # 一些有用的变量\n",
    "        num_features = X.shape[1]\n",
    "\n",
    "        # 您需要正确返回以下变量\n",
    "        best_feature = -1\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        max_info_gain = 0\n",
    "\n",
    "        # 遍历所有特征\n",
    "        for feature in range(num_features): \n",
    "            \n",
    "            # 您的代码：计算在此特征上分割的信息增益\n",
    "            info_gain = \n",
    "            \n",
    "            # 如果信息增益大于目前看到的最大值\n",
    "            if info_gain > max_info_gain:  \n",
    "                # 您的代码：设置 max_info_gain 和 best_feature\n",
    "                max_info_gain = \n",
    "                best_feature = \n",
    "        ### END CODE HERE ##    \n",
    "   \n",
    "    return best_feature\n",
    "    ```\n",
    "    如果您仍然遇到困难，请查看下面的提示。\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> 计算 info_gain 的提示</b></font></summary>\n",
    "        \n",
    "    <code>info_gain = compute_information_gain(X, y, node_indices, feature)</code>\n",
    "    </details>\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>更新 max_info_gain 和 best_feature 的提示</b></font></summary>\n",
    "           <code>max_info_gain = info_gain</code><br>\n",
    "           <code>best_feature = feature</code>\n",
    "    </details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们使用下面的单元格检查您的函数实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)\n",
    "\n",
    "# UNIT TESTS\n",
    "get_best_split_test(get_best_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们在上面看到的，函数返回在根节点处分割的最佳特征是特征 2（\"单独生长\"）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - 构建树\n",
    "\n",
    "在本节中，我们使用您上面实现的函数，通过连续选择最佳分割特征来生成决策树，直到达到停止条件（最大深度为 2）。\n",
    "\n",
    "您不需要为此部分实现任何内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not graded\n",
    "tree = []\n",
    "\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    tree.append((current_depth, branch_name, best_feature, node_indices))\n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [0, 1, 4, 7]\n",
      "  -- Right leaf node with indices [5]\n",
      "- Depth 1, Right: Split on feature: 1\n",
      "  -- Left leaf node with indices [8]\n",
      "  -- Right leaf node with indices [2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
